---
title: "Engineering_Data_Analysis_Class_Project_(ISEN613)"
author: "Ridham"
date: "4/16/2021"
output: github_document
---

```{r, echo=T, results='hide', message=F, warning=F}
#loading / confirming libraries
require(ISLR); 
require(tidyverse); 
require(knitr); 
require(kableExtra)
library(ISLR);
library(leaps);
library(readxl)
require(corrplot)
```

## Read data and preliminary visualization
```{r}
D = read_excel("train.xlsx")
attach(D)
dim(D)
head(D)
pairs(D, cex = 1.5, pch = 1) #pairs plot
```
### Observations:
X1 and X2 seems to be collinear.
X2 and X3 are collinear and non-linear too (no specific pattern though)
No specific pattern for Y1 with any of the predictors

## Correlational plot
```{r}
corr_matrix = cor(D) #Generating correlation matrix for dataframe D
corrplot.mixed(corr_matrix, lower = 'shade', upper = 'pie', order = 'hclust') #correlational plot
```
### Observations:
We can see strong negative correlation between (X1,X2), (X4,X5), (X1,X4) and (X2,X5).
We can see strong positive correlation between (X2,X4) and somewhat strong correlation between (X1,X5)


## Let's first split the data into 70-30 Train-Test data sets.
```{r}
## 70% of the sample size
smpl_size = floor(0.70 * nrow(D))

## set.seed to make the partition reproducible
set.seed(123)
train_ind = sample(seq_len(nrow(D)), size = smpl_size)

#new data
D.train = D[train_ind, ]
D.test = D[-train_ind, ]

#Dimensions of train and test (or validation) set
dim(D.train)
dim(D.test)
```


## Basic Linear Regression Models

```{r}
lm1 = lm(Y1 ~ ., data=D.train) #simple linear regression with all features
summary(lm1) #
```
## Some other models

Now, let's try and test some observations we obtained from exploratory data analysis. Summary of these models shows there is no significant reduction in training error for any case. Therefore, we keep exploring ,and move to interactions between different features in the next section.
```{r}
#Remove X6
lm2 = lm(Y1~.-X6, data=D.train)
summary(lm2)

#Remove X4 and X6
lm3 = lm(Y1~.-X4-X6, data=D.train)
summary(lm3)

#Remove X1, X4, X6
lm4 = lm(Y1~.-X6-X1-X4, data=D.train)
summary(lm4)

#Remove X2, X4, X6
lm5 = lm(Y1~.-X6-X2-X4, data=D.train)
summary(lm5)

#Diagnostic plots
plot(lm3, which=1)
plot(lm3, which=3)
plot(lm3, which=5)
```

## Some more linear models with interactions between features 
We tried many different permutations with different interactions and finally came us with 15 interaction terms. while testing it using test data, error was significantly lower than simple linear regression models. But, this type of models are not easy to interpret and understand the predictions. As our goal of the project was to provide best predictions, we didn't rule out the model.

```{r}
library(caret)
train.control <- trainControl(method = "cv", number = 10)

# Train the model
lm1 = train(Y1~X1+X2+X3+X5+X7+X8, data=D.train, method = "lm", trControl = train.control)
#print
print(lm3)

##interactions
train.control <- trainControl(method = "cv", number = 10)

lm15 = train(Y1~X1+X2+X3+X5+X7+X8+X1*X2+X2*X3+X1*X3+X5*X7+X1*X5+X2*X5+X3*X5+X1*X7+X2*X7+X3*X7+X1*X8+X2*X8+X3*X8+X5*X8+X7*X8, data=D.train, method = "lm", trControl = train.control)
print(lm15)


int.pred = predict(lm15, newdata = D.test)
error.int = mean((int.pred - D.test$Y1)^2)
error.int
```

After tyring and testing enough number of regression models, we decided to go with a systematic approach to deal with importance of different features.

## Best subset selection

```{r}
bss = regsubsets(Y1~.,D, nvmax = length(names(D)))
smry = summary(bss)
names(smry)
smry

plot(smry$adjr2 ,xlab =" Number of Variables ", ylab=" Adjusted R.Sq.",type="l")
num1 = which.max(smry$adjr2)
num1 #number of predictors
points (num1, smry$adjr2[num1], col ="blue",cex =2, pch =20)

plot(smry$bic ,xlab =" Number of Variables ", ylab="bic",type="l")
num1 = which.min(smry$bic)
num1 #number of predictors
points (num1, smry$bic[num1], col ="blue",cex =2, pch =20)

plot(smry$cp ,xlab =" Number of Variables ", ylab=" cp",type="l")
num1 = which.min(smry$cp)
num1 #number of predictors
points (num1, smry$cp[num1], col ="blue",cex =2, pch =20)
```

## Working on results of best subset selection
```{r}
# Define training control
train.control <- trainControl(method = "cv", number = 10)
# Train the model
lm10 = train(Y1~X1+X4+X5+X7+X8, data=D.train, method = "lm", trControl = train.control)
# Summarize the results
print(lm10)

# Define training control
train.control <- trainControl(method = "cv", number = 10)
# Train the model
lm11 = train(Y1~X1+X2+X3+X5+X7+X8, data=D.train, method = "lm", trControl = train.control)
# Summarize the results
print(lm11)

bss.pred = predict(lm11, newdata = D.test)
error.bss = mean((bss.pred - D.test$Y1)^2) #error of best subset selection regression
error.bss
```


## Creating model matrix 
```{r}
#install.packages("glmnet")
#creatig model matrix which is required for Ridge Regression
x = model.matrix(Y1~.,D.train)[,-1] 
y = D.train$Y1

x.test = model.matrix(Y1~.,D.test)[,-1]
y.test = D.test$Y1  
```

# Regularization

## Ridge Regression
```{r}
library(glmnet)
grid = 10^seq(10,-2, length =100)
ridreg = glmnet(x,y,alpha =0, lambda =grid)
```

## Cross-Validation for best lambda Ridge
```{r}
set.seed (123)
cvrid = cv.glmnet(x,y,alpha=0)
plot(cvrid)
bestlamrid = cvrid$lambda.min
bestlamrid
cvrid

#Coefficients
ridout = glmnet(x,y,alpha=0)
predict(ridout, type="coefficients", s=bestlamrid)

rid.pred = predict(ridreg, s=bestlamrid, newx=x.test)
error.ridge = mean((rid.pred - y.test)^2)
error.ridge
```

## Lasso Regression
```{r}
grid = 10^seq(10,-2, length =100)
Lassoreg = glmnet(x,y,alpha =1, lambda =grid)
```

## Cross-Validation for best lambda Lasso
```{r}
set.seed (123)
cvlas = cv.glmnet(x,y,alpha=1)
plot(cvlas)
bestlamlas = cvlas$lambda.min
bestlamlas
cvlas

#Coefficients
lasout = glmnet(x,y,alpha=1)
predict(lasout, type="coefficients", s=bestlamlas)

las.pred.tr = predict(Lassoreg, s=bestlamlas, newx=x)
error.lasso.tr = mean((las.pred.tr - y)^2)
error.lasso.tr

las.pred = predict(Lassoreg, s=bestlamlas, newx=x.test)
error.lasso = mean((las.pred - y.test)^2)
error.lasso
```

# Trees

## Simple tree
```{r}
library(tree)
regtree = tree(Y1~.,D.train)
summary(regtree)
plot(regtree, type = "uniform") #display tree structure
text(regtree,pretty=0)

#Prediction
tree.pred = predict(regtree,D.test)
MSE = mean((D.test$Y1 - tree.pred)^2)
MSE
```

## Tree Pruning

Tree pruning does not improve the model in this case because, pruning does not result any lesser number of leaf-nodes.
```{r}
set.seed(123)

#10 fold Cross validation for tree pruning 
cv.D = cv.tree(regtree, K = 10)

#plotting data
plot(cv.D$size ,cv.D$dev, type="b")
b = cv.D$size[which.min(cv.D$dev)]
b
#pruned tree
prune.D = prune.tree(regtree ,best = b)

#plotting the pruned tree
plot(prune.D,type = "uniform")
text(prune.D ,pretty =0)

#size of pruned tree
summary(prune.D)$size

#prediction
prune.pred=predict(prune.D,D.test)

#test MSE
MSE_prune = mean((D.test$Y1 - prune.pred)^2)
MSE_prune
```
#Bagging

5-fold cross-validation for bagging using caret library.
```{r}
library(caret)
set.seed(23)
cntrl <- trainControl(method = "cv", number = 5)
bagmodel = train(Y1 ~ ., data = D.train, method = "treebag",
        trControl = cntrl)
bag.pred = predict(bagmodel,D.test)
test_MSE_bag = mean((D.test$Y1 - bag.pred)^2)
test_MSE_bag
```

## Random Forest
```{r}
library(randomForest)
set.seed(1)


bag.D = randomForest(Y1~.,data=D.train, mtry=5, importance =TRUE, ntree=500)
bag.D

#prediction
bag.pred = predict(bag.D,D.test)

#test MSE
MSE_bag = mean((D.test$Y1 - bag.pred)^2)
MSE_bag
```
# Boosting: 
First, we tried some random approached. for example, first we fixed n.trees to 5850 and generated different models with different interaction depths. 

## Boosting: Manually tuning interaction depth while keeping other arguments constant.
```{r}
library(gbm)
set.seed(123)
try = seq(1,20, by = 1)
count = 1
MSE_boost = c()

for (i in try){
  boost.D = gbm(Y1~.,data=D.train,
  distribution = "gaussian",n.trees=5850,  shrinkage = 0.1, interaction.depth = i, n.minobsinnode = 5, bag.fraction = 0.7)
  pred.boost = predict(boost.D,newdata=D.test)
  MSE_boost[count] = mean((pred.boost-D.test$Y1)^2)
  count = count + 1
}

## Miracles
try[which.min(MSE_boost)]
min(MSE_boost)
plot(try,MSE_boost, cex = 1, type = "o", col = "blue", main = "interaction depth v/s MSE", xlab = "xlabel", ylab = "ylabel", pch = 20)

MSE_boost
```

## Support Vector Machines

Gives mediocre results on test data.
```{r}
#Install Package
# install.packages("e1071")

#Load Library
library(e1071)
 
#Regression with SVM
modelsvm = svm(Y1~.,D.train, kernel = "linear")

#Predict using SVM regression
predYsvm = predict(modelsvm, D.test)
mean((predYsvm-D.test$Y1)^2)
```

